================================================================================
PyTorch Attention Benchmark on Telerun GPU
================================================================================

GPU: NVIDIA RTX 4000 Ada Generation
CUDA Version: 12.8
PyTorch Version: 2.8.0+cu128
GPU Memory: 21.0 GB

================================================================================
hdim=512: bs=1, nh=4, seq=64, hdim=512
================================================================================

PyTorch Optimized:
  Time: 0.032 ms
  Performance: 1.04 TFLOPS

PyTorch Naive:
  Time: 0.077 ms
  Performance: 0.43 TFLOPS
  Speedup: 2.41x

================================================================================
hdim=2048: bs=1, nh=4, seq=64, hdim=2048
================================================================================

PyTorch Optimized:
  Time: 0.114 ms
  Performance: 1.18 TFLOPS

PyTorch Naive:
  Time: 0.219 ms
  Performance: 0.61 TFLOPS
  Speedup: 1.92x

================================================================================
hdim=4096: bs=1, nh=2, seq=32, hdim=4096
================================================================================

PyTorch Optimized:
  Time: 0.190 ms
  Performance: 0.18 TFLOPS

PyTorch Naive:
  Time: 0.412 ms
  Performance: 0.08 TFLOPS
  Speedup: 2.17x

================================================================================
hdim=8192: bs=1, nh=1, seq=16, hdim=8192
================================================================================

PyTorch Optimized:
  Time: 0.362 ms
  Performance: 0.02 TFLOPS

PyTorch Naive:
  Time: 0.081 ms
  Performance: 0.10 TFLOPS
  Speedup: 0.22x


================================================================================
SUMMARY: PyTorch on This GPU
================================================================================

Config          Optimized       Naive           Speedup    TFLOPS    
----------------------------------------------------------------------
hdim=512             0.032 ms       0.077 ms    2.41x      1.04
hdim=2048            0.114 ms       0.219 ms    1.92x      1.18
hdim=4096            0.190 ms       0.412 ms    2.17x      0.18
hdim=8192            0.362 ms       0.081 ms    0.22x      0.02
================================================================================
PyTorch Benchmark Wrapper
================================================================================

Writing embedded Python script...
Executing PyTorch benchmark...


================================================================================
PyTorch Benchmark Complete
================================================================================
