================================================================================
PyTorch Attention Benchmark on Telerun GPU
================================================================================

GPU: NVIDIA RTX 4000 Ada Generation
CUDA Version: 12.8
PyTorch Version: 2.8.0+cu128
GPU Memory: 21.0 GB

================================================================================
hdim=512: bs=1, nh=4, seq=64, hdim=512
================================================================================

PyTorch Optimized:
  Time: 0.032 ms
  Performance: 1.04 TFLOPS

PyTorch Naive:
  Time: 0.148 ms
  Performance: 0.23 TFLOPS
  Speedup: 4.61x

================================================================================
hdim=2048: bs=1, nh=4, seq=64, hdim=2048
================================================================================

PyTorch Optimized:
  Time: 0.114 ms
  Performance: 1.18 TFLOPS

PyTorch Naive:
  Time: 0.219 ms
  Performance: 0.61 TFLOPS
  Speedup: 1.92x

================================================================================
hdim=4096: bs=1, nh=2, seq=32, hdim=4096
================================================================================

PyTorch Optimized:
  Time: 0.190 ms
  Performance: 0.18 TFLOPS

PyTorch Naive:
  Time: 0.412 ms
  Performance: 0.08 TFLOPS
  Speedup: 2.17x

================================================================================
hdim=8192: bs=1, nh=1, seq=16, hdim=8192
================================================================================

PyTorch Optimized:
  Time: 0.363 ms
  Performance: 0.02 TFLOPS

PyTorch Naive:
  Time: 0.084 ms
  Performance: 0.10 TFLOPS
  Speedup: 0.23x


================================================================================
SUMMARY: PyTorch on This GPU
================================================================================

Config          Optimized       Naive           Speedup    TFLOPS    
----------------------------------------------------------------------
hdim=512             0.032 ms       0.148 ms    4.61x      1.04
hdim=2048            0.114 ms       0.219 ms    1.92x      1.18
hdim=4096            0.190 ms       0.412 ms    2.17x      0.18
hdim=8192            0.363 ms       0.084 ms    0.23x      0.02

================================================================================
EXPECTED CUDA KERNEL RESULTS
================================================================================

Config          Expected        Speedup vs Baseline 
-------------------------------------------------------
hdim=512        0.062 ms        1.91x               
hdim=2048       0.214 ms        2.02x               
hdim=4096       0.126 ms        1.48x               
hdim=8192       0.194 ms        1.03x               

================================================================================
COMPARISON
================================================================================

Compare PyTorch Optimized times above with CUDA kernel times.

To run the CUDA kernel:
  python ../telerun/telerun.py submit attention_kernel.cu

Look for 'Optimized kernel time' in the output and compare.
================================================================================
================================================================================
PyTorch Benchmark Wrapper
================================================================================

Writing embedded Python script...
Executing PyTorch benchmark...


================================================================================
PyTorch Benchmark Complete
================================================================================
