================================================================================
PyTorch Attention Benchmark on Telerun GPU
================================================================================

GPU: NVIDIA RTX 4000 Ada Generation
CUDA Version: 12.8
PyTorch Version: 2.8.0+cu128
GPU Memory: 21.0 GB

================================================================================
hdim=512: bs=1, nh=4, seq=64, hdim=512
================================================================================

PyTorch Optimized:
  Time: 0.032 ms
  Performance: 1.05 TFLOPS

PyTorch Naive:
  Time: 0.077 ms
  Performance: 0.44 TFLOPS
  Speedup: 2.40x

================================================================================
hdim=2048: bs=1, nh=4, seq=64, hdim=2048
================================================================================

PyTorch Optimized:
  Time: 0.114 ms
  Performance: 1.18 TFLOPS

PyTorch Naive:
  Time: 0.219 ms
  Performance: 0.61 TFLOPS
  Speedup: 1.93x

================================================================================
hdim=4096: bs=1, nh=2, seq=32, hdim=4096
================================================================================

PyTorch Optimized:
  Time: 0.190 ms
  Performance: 0.18 TFLOPS

PyTorch Naive:
  Time: 0.407 ms
  Performance: 0.08 TFLOPS
  Speedup: 2.15x

================================================================================
hdim=8192: bs=1, nh=1, seq=16, hdim=8192
================================================================================

PyTorch Optimized:
  Time: 0.338 ms
  Performance: 0.02 TFLOPS

PyTorch Naive:
  Time: 0.084 ms
  Performance: 0.10 TFLOPS
  Speedup: 0.25x


================================================================================
SUMMARY: PyTorch on This GPU
================================================================================

Config          Optimized       Naive           Speedup    TFLOPS    
----------------------------------------------------------------------
hdim=512             0.032 ms       0.077 ms    2.40x      1.05
hdim=2048            0.114 ms       0.219 ms    1.93x      1.18
hdim=4096            0.190 ms       0.407 ms    2.15x      0.18
hdim=8192            0.338 ms       0.084 ms    0.25x      0.02
================================================================================
PyTorch Benchmark Wrapper
================================================================================

Writing embedded Python script...
Executing PyTorch benchmark...


================================================================================
PyTorch Benchmark Complete
================================================================================
