=================================================
RoPE-Fused Attention Kernel Test Suite
=================================================

========================================
DEBUG: Isolating RoPE bug
========================================

RoPE cache for position 0:
  d=0: cos=1.000000, sin=0.000000
  d=1: cos=1.000000, sin=0.000000
  d=2: cos=1.000000, sin=0.000000
  d=3: cos=1.000000, sin=0.000000
  d=4: cos=1.000000, sin=0.000000
  d=5: cos=1.000000, sin=0.000000
  d=6: cos=1.000000, sin=0.000000
  d=7: cos=1.000000, sin=0.000000
RoPE cache for position 1:
  d=0: cos=0.540302, sin=0.841471
  d=1: cos=0.540302, sin=0.841471
  d=2: cos=0.647906, sin=0.761720
  d=3: cos=0.647906, sin=0.761720
  d=4: cos=0.731761, sin=0.681561
  d=5: cos=0.731761, sin=0.681561
  d=6: cos=0.796458, sin=0.604694
  d=7: cos=0.796458, sin=0.604694

Q_rope for query 0 (should be ~0.1 since pos=0 means identity):
  d=0: 0.100000
  d=1: 0.100000
  d=2: 0.100000
  d=3: 0.100000
  d=4: 0.100000
  d=5: 0.100000
  d=6: 0.100000
  d=7: 0.100000

K_rope for key 0 (should be ~0.1):
  d=0: 0.100000
  d=1: 0.100000
  d=2: 0.100000
  d=3: 0.100000
  d=4: 0.100000
  d=5: 0.100000
  d=6: 0.100000
  d=7: 0.100000

K_rope for key 1 (should be rotated):
  d=0: -0.030117
  d=1: 0.138177
  d=2: -0.011381
  d=3: 0.140963
  d=4: 0.005020
  d=5: 0.141332
  d=6: 0.019176
  d=7: 0.140115

CPU: score(q=0, k=0) = 1.279999 (before scale)

Output comparison for query 0:
  dim |    GPU    |    CPU    |   diff   | ratio
  ----|-----------|-----------|----------|------

--- BASIC CORRECTNESS TESTS ---

========================================
Testing RoPE-fused attention kernel:
  Batch size: 1
  Num heads: 1
  Sequence length: 4
  Head dimension: 8
========================================

Running RoPE-fused GPU kernel...
Running CPU reference (RoPE + attention)...

--- Performance Results ---
RoPE-fused kernel time: 0.009 ms

Checking results vs CPU...
  Max difference: 0.000000 (within tolerance 0.050000)

✓ TEST PASSED

========================================
Testing RoPE-fused attention kernel:
  Batch size: 1
  Num heads: 2
  Sequence length: 8
  Head dimension: 16
========================================

Running RoPE-fused GPU kernel...
Running CPU reference (RoPE + attention)...

--- Performance Results ---
RoPE-fused kernel time: 0.008 ms

Checking results vs CPU...
  Max difference: 0.000000 (within tolerance 0.050000)

✓ TEST PASSED

========================================
Testing RoPE-fused attention kernel:
  Batch size: 2
  Num heads: 4
  Sequence length: 32
  Head dimension: 32
========================================

Running RoPE-fused GPU kernel...
Running CPU reference (RoPE + attention)...

--- Performance Results ---
RoPE-fused kernel time: 0.015 ms

Checking results vs CPU...
  Max difference: 0.000000 (within tolerance 0.050000)

✓ TEST PASSED

--- LARGER DIMENSION TESTS ---

========================================
Testing RoPE-fused attention kernel:
  Batch size: 1
  Num heads: 4
  Sequence length: 16
  Head dimension: 64
========================================

Running RoPE-fused GPU kernel...
Running CPU reference (RoPE + attention)...

--- Performance Results ---
RoPE-fused kernel time: 0.010 ms

Checking results vs CPU...
  Max difference: 0.000000 (within tolerance 0.050000)

✓ TEST PASSED

========================================
Testing RoPE-fused attention kernel:
  Batch size: 1
  Num heads: 4
  Sequence length: 64
  Head dimension: 128
========================================

Running RoPE-fused GPU kernel...
Running CPU reference (RoPE + attention)...

--- Performance Results ---
RoPE-fused kernel time: 0.053 ms

Checking results vs CPU...
  Max difference: 0.000000 (within tolerance 0.050000)

✓ TEST PASSED

--- TARGET DIMENSION TESTS ---

========================================
Testing RoPE-fused attention kernel:
  Batch size: 1
  Num heads: 4
  Sequence length: 64
  Head dimension: 512
========================================

Running RoPE-fused GPU kernel...
Running CPU reference (RoPE + attention)...

--- Performance Results ---
RoPE-fused kernel time: 0.177 ms

Checking results vs CPU...
  Max difference: 0.000000 (within tolerance 0.050000)

✓ TEST PASSED

========================================
Testing RoPE-fused attention kernel:
  Batch size: 1
  Num heads: 4
  Sequence length: 64
  Head dimension: 2048
========================================

Running RoPE-fused GPU kernel...
Running CPU reference (RoPE + attention)...

--- Performance Results ---
RoPE-fused kernel time: 0.749 ms

Checking results vs CPU...
  Max difference: 0.000000 (within tolerance 0.050000)

✓ TEST PASSED

========================================
Testing RoPE-fused attention kernel:
  Batch size: 1
  Num heads: 2
  Sequence length: 32
  Head dimension: 4096
========================================

Running RoPE-fused GPU kernel...
Running CPU reference (RoPE + attention)...

--- Performance Results ---
RoPE-fused kernel time: 0.225 ms

Checking results vs CPU...
  Max difference: 0.000000 (within tolerance 0.050000)

✓ TEST PASSED

========================================
Testing RoPE-fused attention kernel:
  Batch size: 1
  Num heads: 1
  Sequence length: 16
  Head dimension: 8192
========================================

Running RoPE-fused GPU kernel...
Running CPU reference (RoPE + attention)...

--- Performance Results ---
RoPE-fused kernel time: 0.268 ms

Checking results vs CPU...
  Max difference: 0.000001 (within tolerance 0.050000)

✓ TEST PASSED

--- FUSED vs SEPARATE BENCHMARK (PARTIAL ROTATION) ---
Testing realistic LLaMA-style configs:
  hdim=128, rotary_dim=64   (50% rotation)
  hdim=512, rotary_dim=128  (25% rotation)
  hdim=2048, rotary_dim=128 (6% rotation)
  hdim=4096, rotary_dim=128 (3% rotation)
  hdim=8192, rotary_dim=128 (1.5% rotation)


========================================
BENCHMARK: Fused vs Separate RoPE+Attention
  bs=1, nh=4, seq=64, hdim=128, rotary_dim=64
========================================

  V6 (full fusion):         0.054 ms  [1.74x slower]
  V7 (hybrid):              0.019 ms  [1.61x FASTER] ✓
  Separate (baseline):      0.031 ms

========================================
BENCHMARK: Fused vs Separate RoPE+Attention
  bs=1, nh=4, seq=64, hdim=512, rotary_dim=128
========================================

  V6 (full fusion):         0.158 ms  [2.33x slower]
  V7 (hybrid):              0.047 ms  [1.44x FASTER] ✓
  Separate (baseline):      0.068 ms

========================================
BENCHMARK: Fused vs Separate RoPE+Attention
  bs=1, nh=4, seq=64, hdim=2048, rotary_dim=128
========================================

  V6 (full fusion):         0.562 ms  [2.41x slower]
  V7 (hybrid):              0.166 ms  [1.40x FASTER] ✓
  Separate (baseline):      0.233 ms

========================================
BENCHMARK: Fused vs Separate RoPE+Attention
  bs=1, nh=2, seq=32, hdim=4096, rotary_dim=128
========================================
CUDA error 1 (invalid argument) at /tmp/tmpr_vk8ovg/source.cu:900
