=================================================
BF16 ATTENTION KERNELS - COMPREHENSIVE TEST
=================================================


╔═══════════════════════════════════════════════════╗
║  TESTING hdim=512 (seq=64)                     ║
╚═══════════════════════════════════════════════════╝

========================================
Testing BASIC ATTENTION (BF16):
  bs=1, nh=4, seq=64, hdim=512
========================================

Time: 0.069 ms
Performance: 0.49 TFLOPS

========================================
Testing ROPE-FUSED ATTENTION (BF16):
  bs=1, nh=4, seq=64, hdim=512
========================================

Time: 0.119 ms
Performance: 0.28 TFLOPS

========================================
Testing SINUSOIDAL-FUSED ATTENTION (BF16):
  bs=1, nh=4, seq=64, hdim=512
========================================

Time: 0.130 ms
Performance: 0.26 TFLOPS


╔═══════════════════════════════════════════════════╗
║  TESTING hdim=2048 (seq=64)                     ║
╚═══════════════════════════════════════════════════╝

========================================
Testing BASIC ATTENTION (BF16):
  bs=1, nh=4, seq=64, hdim=2048
========================================

Time: 0.248 ms
Performance: 0.54 TFLOPS

========================================
Testing ROPE-FUSED ATTENTION (BF16):
  bs=1, nh=4, seq=64, hdim=2048
========================================

Time: 0.450 ms
Performance: 0.30 TFLOPS

========================================
Testing SINUSOIDAL-FUSED ATTENTION (BF16):
  bs=1, nh=4, seq=64, hdim=2048
========================================

Time: 0.465 ms
Performance: 0.29 TFLOPS


╔═══════════════════════════════════════════════════╗
║  TESTING hdim=4096 (seq=32)                     ║
╚═══════════════════════════════════════════════════╝

========================================
Testing BASIC ATTENTION (BF16):
  bs=1, nh=2, seq=32, hdim=4096
========================================

Time: 0.134 ms
Performance: 0.25 TFLOPS

========================================
Testing ROPE-FUSED ATTENTION (BF16):
  bs=1, nh=2, seq=32, hdim=4096
========================================

Time: 0.212 ms
Performance: 0.16 TFLOPS

========================================
Testing SINUSOIDAL-FUSED ATTENTION (BF16):
  bs=1, nh=2, seq=32, hdim=4096
========================================

Time: 0.226 ms
Performance: 0.15 TFLOPS


╔═══════════════════════════════════════════════════╗
║  TESTING hdim=8192 (seq=16)                     ║
╚═══════════════════════════════════════════════════╝

========================================
Testing BASIC ATTENTION (BF16):
  bs=1, nh=1, seq=16, hdim=8192
========================================

Time: 0.168 ms
Performance: 0.05 TFLOPS

========================================
Testing ROPE-FUSED ATTENTION (BF16):
  bs=1, nh=1, seq=16, hdim=8192
========================================

Time: 0.276 ms
Performance: 0.03 TFLOPS

========================================
Testing SINUSOIDAL-FUSED ATTENTION (BF16):
  bs=1, nh=1, seq=16, hdim=8192
========================================

Time: 0.294 ms
Performance: 0.03 TFLOPS


=================================================
ALL BF16 TESTS COMPLETED!
=================================================

Summary:
  - Tested 3 kernel variants in BF16
  - Tested 4 hidden dimensions: 512, 2048, 4096, 8192
  - BF16 should provide better stability than FP16
  - BF16 has 2x less memory vs FP32
=================================================
