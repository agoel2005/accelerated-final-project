================================================================================
PyTorch Attention Benchmark on Telerun GPU
================================================================================

GPU: NVIDIA RTX 4000 Ada Generation
CUDA Version: 12.8
PyTorch Version: 2.8.0+cu128
GPU Memory: 21.0 GB

================================================================================
hdim=512: bs=1, nh=4, seq=64, hdim=512
================================================================================

PyTorch Optimized:
  Time: 0.032 ms
  Performance: 1.05 TFLOPS

PyTorch Naive:
  Time: 0.092 ms
  Performance: 0.36 TFLOPS
  Speedup: 2.88x

================================================================================
hdim=2048: bs=1, nh=4, seq=64, hdim=2048
================================================================================

PyTorch Optimized:
  Time: 0.114 ms
  Performance: 1.18 TFLOPS

PyTorch Naive:
  Time: 0.220 ms
  Performance: 0.61 TFLOPS
  Speedup: 1.93x

================================================================================
hdim=4096: bs=1, nh=2, seq=32, hdim=4096
================================================================================

PyTorch Optimized:
  Time: 0.190 ms
  Performance: 0.18 TFLOPS

PyTorch Naive:
  Time: 0.412 ms
  Performance: 0.08 TFLOPS
  Speedup: 2.17x

================================================================================
hdim=8192: bs=1, nh=1, seq=16, hdim=8192
================================================================================

PyTorch Optimized:
  Time: 0.363 ms
  Performance: 0.02 TFLOPS

PyTorch Naive:
  Time: 0.090 ms
  Performance: 0.09 TFLOPS
  Speedup: 0.25x


================================================================================
SUMMARY: PyTorch on This GPU
================================================================================

Config          Optimized       Naive           Speedup    TFLOPS    
----------------------------------------------------------------------
hdim=512             0.032 ms       0.092 ms    2.88x      1.05
hdim=2048            0.114 ms       0.220 ms    1.93x      1.18
hdim=4096            0.190 ms       0.412 ms    2.17x      0.18
hdim=8192            0.363 ms       0.090 ms    0.25x      0.02

================================================================================
EXPECTED CUDA KERNEL RESULTS
================================================================================

Config          Expected        Speedup vs Baseline 
-------------------------------------------------------
hdim=512        0.062 ms        1.91x               
hdim=2048       0.214 ms        2.02x               
hdim=4096       0.126 ms        1.48x               
hdim=8192       0.194 ms        1.03x               

================================================================================
COMPARISON
================================================================================

Compare PyTorch Optimized times above with CUDA kernel times.

To run the CUDA kernel:
  python ../telerun/telerun.py submit attention_kernel.cu

Look for 'Optimized kernel time' in the output and compare.
================================================================================
================================================================================
PyTorch Benchmark Wrapper
================================================================================

Writing embedded Python script...
Executing PyTorch benchmark...


================================================================================
PyTorch Benchmark Complete
================================================================================
