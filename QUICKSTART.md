# Quick Start Guide

## Using the Optimized Attention Kernel

This guide shows you how to use our optimized CUDA attention kernel in your project.

---

## TL;DR

**File to use:** `attention_kernel.cu`
**Performance:** 1.48x-2.02x faster than baseline for large hidden dimensions
**Status:** Production-ready, all tests passing ‚úÖ

---

## Basic Usage

### 1. Compile

```bash
nvcc -o attention_kernel attention_kernel.cu -O3 -arch=sm_80
```

**Architecture flags:**
- `-arch=sm_80`: A100 GPUs
- `-arch=sm_86`: RTX 3090, A6000
- `-arch=sm_89`: RTX 4090, L40
- `-arch=sm_90`: H100

### 2. Run Tests

```bash
./attention_kernel
```

This runs comprehensive tests including:
- Small dimensions (correctness validation)
- Large dimensions (512, 2048, 4096, 8192)
- Cross-attention tests
- Performance benchmarks

### 3. Integrate Into Your Code

```cpp
#include "attention_kernel.cu"  // Include the file

// Your data (on GPU)
float *d_Q, *d_K, *d_V, *d_out;

// Call the optimized attention forward
attention_forward_optimized(
    d_Q,        // Query tensor [batch √ó heads √ó seq_q √ó head_dim]
    d_K,        // Key tensor [batch √ó heads √ó seq_k √ó head_dim]
    d_V,        // Value tensor [batch √ó heads √ó seq_k √ó head_dim]
    d_out,      // Output tensor [batch √ó heads √ó seq_q √ó head_dim]
    batch_sz,   // Batch size
    num_heads,  // Number of attention heads
    len_q,      // Query sequence length
    len_k,      // Key/Value sequence length
    head_d      // Head dimension (512, 2048, 4096, or 8192 recommended)
);
```

---

## Memory Layout

**All tensors are in row-major format:**

```
Q: [batch_sz][num_heads][len_q][head_d]
K: [batch_sz][num_heads][len_k][head_d]
V: [batch_sz][num_heads][len_k][head_d]
out: [batch_sz][num_heads][len_q][head_d]

Linear indexing:
Q[b,h,i,d] ‚Üí Q[(b*num_heads + h)*len_q*head_d + i*head_d + d]
```

**Memory must be:**
- Allocated on GPU (`cudaMalloc`)
- Contiguous
- Aligned to at least 4-byte boundaries (16-byte preferred for float4)

---

## Performance Tips

### 1. Choose Optimal Batch Sizes

**Current performance:**
```
Small batches (16-64 queries): 1.48x-2.02x speedup ‚úì
Large batches (512+ queries):  Expected 2-4x speedup üöÄ
```

**Recommendation:** Batch as many queries as possible to maximize GPU utilization.

### 2. Use Large Hidden Dimensions

The optimized kernel is specifically designed for:
```
‚úÖ head_d ‚â• 512 (online softmax with tiling)
‚ö†Ô∏è  head_d < 512 (uses baseline kernel)
```

### 3. Profile Your Workload

```bash
# System profiling
nsys profile --stats=true ./your_program

# Detailed kernel metrics
ncu --set full ./your_program
```

### 4. Ensure Memory Alignment

```cpp
// Good: Aligned allocation
float* d_Q;
cudaMalloc(&d_Q, size * sizeof(float));  // Automatically aligned

// Bad: Misaligned offset
float* d_Q_offset = d_Q + 1;  // Not 16-byte aligned!
attention_forward_optimized(d_Q_offset, ...);  // May be slower
```

---

## API Reference

### `attention_forward_optimized`

**Signature:**
```cpp
void attention_forward_optimized(
    const float* Q,         // Input: Query tensor
    const float* K,         // Input: Key tensor
    const float* V,         // Input: Value tensor
    float* out,             // Output: Attention output
    int batch_sz,           // Batch size
    int num_heads,          // Number of attention heads
    int len_q,              // Query sequence length
    int len_k,              // Key/Value sequence length
    int head_d              // Head dimension
);
```

**Parameters:**
- `Q, K, V`: Input tensors (GPU pointers, row-major layout)
- `out`: Output tensor (GPU pointer, same layout as Q)
- `batch_sz`: Batch dimension
- `num_heads`: Number of parallel attention heads
- `len_q`: Number of query tokens
- `len_k`: Number of key/value tokens
- `head_d`: Dimension of each head (512, 2048, 4096, 8192 recommended)

**Returns:** `void` (writes results to `out`)

**Computes:**
```
scores = softmax(Q @ K^T / sqrt(head_d))
out = scores @ V
```

**Error Handling:**
- Calls `CUDA_CHECK` internally
- Prints error and exits on CUDA errors
- Synchronizes device after kernel launch

---

## Architecture Details

### Kernel Selection

```cpp
if (head_d >= 512) {
    // Use optimized kernel with online softmax
    attention_fwd_kernel_large_hdim<<<...>>>(...)
} else {
    // Use baseline kernel
    attention_fwd_kernel<<<...>>>(...)
}
```

### Optimized Kernel Features

1. **Online Softmax with K/V Tiling**
   - Processes K,V in blocks of 64 elements
   - Streaming computation reduces memory pressure
   - Numerically stable with correction factors

2. **Float4 Vectorization**
   - 4x memory bandwidth per instruction
   - 128-bit aligned loads
   - Better cache utilization

3. **Warp-Level Reductions**
   - Fast parallel max/sum computation
   - No atomic operations
   - Efficient shared memory usage

4. **Grid Configuration**
   ```
   Grid: [len_q, batch_sz √ó num_heads]
   Block: 256 threads
   Shared memory: (64 + 32) √ó 4 bytes = 384 bytes
   ```

---

## Troubleshooting

### Common Issues

#### 1. **Correctness Errors**

**Symptom:** Output differs from CPU reference
```
Max difference: 0.523401 - ‚úó FAILED
```

**Solutions:**
- Check memory layout (must be contiguous, row-major)
- Verify dimensions (batch, heads, seq_len, head_dim)
- Ensure data is on GPU (not CPU pointers)
- Check for NaN/Inf in input data

#### 2. **CUDA Errors**

**Symptom:**
```
CUDA error 1 (invalid argument) at line 305
```

**Solutions:**
- Check all pointers are valid GPU memory
- Verify dimensions are positive
- Ensure sufficient GPU memory available
- Check kernel launch configuration

#### 3. **Poor Performance**

**Symptom:** Slower than expected

**Solutions:**
- Use batch sizes ‚â• 64 for better GPU utilization
- Ensure head_d ‚â• 512 to trigger optimized path
- Profile with `nsys`/`ncu` to find bottleneck
- Check for memory throttling (GPU too hot)
- Verify using correct GPU (not CPU fallback)

#### 4. **Compilation Errors**

**Symptom:**
```
error: identifier "__shfl_xor_sync" is undefined
```

**Solutions:**
- Use CUDA 9.0+ (warp intrinsics)
- Specify correct `-arch` flag (sm_70+)
- Include `<cuda_runtime.h>`

---

## Examples

### Example 1: Single Head, Small Batch

```cpp
int batch_sz = 1;
int num_heads = 1;
int seq_len = 64;
int head_d = 2048;

// Allocate GPU memory
float *d_Q, *d_K, *d_V, *d_out;
size_t size = batch_sz * num_heads * seq_len * head_d * sizeof(float);
cudaMalloc(&d_Q, size);
cudaMalloc(&d_K, size);
cudaMalloc(&d_V, size);
cudaMalloc(&d_out, size);

// Initialize data (example: random)
// ... (copy data from CPU to GPU) ...

// Run attention
attention_forward_optimized(
    d_Q, d_K, d_V, d_out,
    batch_sz, num_heads, seq_len, seq_len, head_d
);
cudaDeviceSynchronize();

// Copy result back
// ... (copy d_out to CPU) ...

// Cleanup
cudaFree(d_Q);
cudaFree(d_K);
cudaFree(d_V);
cudaFree(d_out);
```

### Example 2: Multi-Head, Large Batch

```cpp
int batch_sz = 8;
int num_heads = 16;
int seq_len = 512;
int head_d = 4096;

size_t size = batch_sz * num_heads * seq_len * head_d * sizeof(float);
float *d_Q, *d_K, *d_V, *d_out;
cudaMalloc(&d_Q, size);
cudaMalloc(&d_K, size);
cudaMalloc(&d_V, size);
cudaMalloc(&d_out, size);

// ... initialize data ...

// Benchmark
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

cudaEventRecord(start);
for (int i = 0; i < 100; i++) {
    attention_forward_optimized(
        d_Q, d_K, d_V, d_out,
        batch_sz, num_heads, seq_len, seq_len, head_d
    );
}
cudaEventRecord(stop);
cudaEventSynchronize(stop);

float ms = 0;
cudaEventElapsedTime(&ms, start, stop);
printf("Average time: %.3f ms\n", ms / 100);

cudaFree(d_Q);
cudaFree(d_K);
cudaFree(d_V);
cudaFree(d_out);
cudaEventDestroy(start);
cudaEventDestroy(stop);
```

### Example 3: Cross-Attention

```cpp
int batch_sz = 4;
int num_heads = 8;
int len_q = 128;   // Query length
int len_k = 256;   // Key/Value length (different!)
int head_d = 2048;

// Q has shape [batch_sz, num_heads, len_q, head_d]
// K,V have shape [batch_sz, num_heads, len_k, head_d]
size_t size_q = batch_sz * num_heads * len_q * head_d * sizeof(float);
size_t size_kv = batch_sz * num_heads * len_k * head_d * sizeof(float);

float *d_Q, *d_K, *d_V, *d_out;
cudaMalloc(&d_Q, size_q);
cudaMalloc(&d_K, size_kv);
cudaMalloc(&d_V, size_kv);
cudaMalloc(&d_out, size_q);

// ... initialize data ...

// Cross-attention: Q from one sequence, K,V from another
attention_forward_optimized(
    d_Q, d_K, d_V, d_out,
    batch_sz, num_heads,
    len_q,    // Query length
    len_k,    // Key/Value length (can differ!)
    head_d
);

cudaFree(d_Q);
cudaFree(d_K);
cudaFree(d_V);
cudaFree(d_out);
```

---

## Benchmarking Against PyTorch

To compare with PyTorch's optimized attention:

```python
import torch
import torch.nn.functional as F
import time

# Configuration
batch_sz = 1
num_heads = 4
seq_len = 64
head_d = 2048

# Create tensors
q = torch.randn(batch_sz, num_heads, seq_len, head_d, device='cuda')
k = torch.randn(batch_sz, num_heads, seq_len, head_d, device='cuda')
v = torch.randn(batch_sz, num_heads, seq_len, head_d, device='cuda')

# Warmup
for _ in range(10):
    out = F.scaled_dot_product_attention(q, k, v)
torch.cuda.synchronize()

# Benchmark
start = torch.cuda.Event(enable_timing=True)
end = torch.cuda.Event(enable_timing=True)
start.record()
for _ in range(100):
    out = F.scaled_dot_product_attention(q, k, v)
end.record()
torch.cuda.synchronize()

print(f'PyTorch time: {start.elapsed_time(end)/100:.3f} ms')

# Compare with our kernel:
# Our kernel (hdim=2048): 0.214 ms
# If PyTorch is faster, they're using FlashAttention-2 or similar
# If similar, we're near-optimal!
```

---

## Performance Expectations

### Speedup vs Baseline

| Configuration | Expected Speedup |
|---------------|------------------|
| hdim=512, seq=64 | 1.8x-2.0x |
| hdim=2048, seq=64 | 1.9x-2.1x |
| hdim=4096, seq=32 | 1.4x-1.6x |
| hdim=8192, seq=16 | 1.0x-1.1x (memory bandwidth limit) |

### Absolute Performance

| Configuration | Estimated TFLOPS | Time (ms) |
|---------------|------------------|-----------|
| hdim=512, bs=1, nh=4, seq=64 | 2.1 | 0.06 |
| hdim=2048, bs=1, nh=4, seq=64 | 2.5 | 0.21 |
| hdim=4096, bs=1, nh=2, seq=32 | 4.3 | 0.13 |
| hdim=8192, bs=1, nh=1, seq=16 | 0.7 | 0.19 |

**Note:** Actual performance depends on GPU model and system configuration.

---

## Additional Resources

- **Full Documentation:** See `FINAL_SUMMARY.md`
- **Optimization Details:** See `OPTIMIZATION_LOG.md`
- **Performance Analysis:** See `PERFORMANCE_ANALYSIS.md`
- **Bottleneck Analysis:** See `bottleneck_analysis.md`

---

## Support

For issues or questions:
1. Check `FINAL_SUMMARY.md` for known issues
2. Review `OPTIMIZATION_LOG.md` for implementation details
3. Run tests to verify correctness: `./attention_kernel`

---

*Last Updated: 2025-11-25*
